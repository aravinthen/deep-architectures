# Program:     transformer.py
# Author:      Aravinthen Rajkumar
# Description: A reimplementation of the transformer from Vaswani et al. 2017

import torch
import torch.nn
import math


class Encoder(nn.Module):
    def __init__(self, ):
        super().__init__()

class Decoder(nn.Module):
    def __init__(self, ):
        super().__init__()

class MultiHeadAttention(nn.Module):
    def __init__(self, ):
        super().__init__()

class Encoding(nn.Module):
    def __init__(self, ):
        super().__init__()

class Transformer(nn.Module):
    """
    This is a transformer written from scratch, intended to be as close to the 
    model demonstrated in Attention is All You Need.
    """
    def __init__(self, ):
        super().__init__()
